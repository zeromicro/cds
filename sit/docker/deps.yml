version: '2.4'
services:
  mysql:
    image: mysql:8
    container_name: mysql
    ports:
      - "3307:3306"
    volumes:
      - ../../third_party/mysql/init/init.sql:/docker-entrypoint-initdb.d/test.sql
      - ../../third_party/mysqld.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf
    restart: always
    environment:
       MYSQL_ROOT_PASSWORD: root
    # networks:
    #   - bridge
    healthcheck:
      test: ["CMD", "mysql" ,"-h", "mysql", "-P", "3306", "-u", "root", "-proot" , "-e", "SELECT 1"]
      interval: 2s
      timeout: 5s
      retries: 30
  etcd:
    image: bitnami/etcd:3.4.13
    container_name: etcd
    ports:
      - "2379:2379"
      - "2380:2380"
    environment:
      - ALLOW_NONE_AUTHENTICATION=yes
      - ETCD_ADVERTISE_CLIENT_URLS=http://etcd:2379
    healthcheck:
      test: ["CMD", "etcdctl", "member", "list", '--endpoints="localhost:2379"']
      interval: 2s
      timeout: 5s
      retries: 30
  redis:
    image: redis:latest
    container_name: redis
    ports:
      - 6379:6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 1s
      timeout: 3s
      retries: 30
  zookeeper:
    image: zookeeper:3.6
    container_name: zookeeper
    ports:
      - 2181:2181
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 2s
      timeout: 5s
      retries: 30
    environment:
      # The number of milliseconds of each tick
      - ZOO_TICK_TIME=2000
      # The number of ticks that the initial
      # synchronization phase can take
      # This value is not quite motivated
      - ZOO_INIT_LIMIT=300 
      # The number of ticks that can pass between
      # sending a request and getting an acknowledgement
      - ZOO_SYNC_LIMIT=10
      - ZOO_MAX_CLIENT_CNXNS=2000
      - ZOO_AUTOPURGE_SNAPRETAINCOUNT=10
      - ZOO_AUTOPURGE_PURGEINTERVAL=1
      # To avoid seeks ZooKeeper allocates space in the transaction log file in
      # blocks of preAllocSize kilobytes. The default block size is 64M. One reason
      # for changing the size of the blocks is to reduce the block size if snapshots
      # are taken more often. (Also, see snapCount).
      - ZOO_PRE_ALLOC_SIZE=131072
      # Clients can submit requests faster than ZooKeeper can process them,
      # especially if there are a lot of clients. To prevent ZooKeeper from running
      # out of memory due to queued requests, ZooKeeper will throttle clients so that
      # there is no more than globalOutstandingLimit outstanding requests in the
      # system. The default limit is 1,000.ZooKeeper logs transactions to a
      # transaction log. After snapCount transactions are written to a log file a
      # snapshot is started and a new transaction log file is started. The default
      # snapCount is 10,000.
      - ZOO_SNAP_COUNT=3000000
      # - ZOO_STANDALONE_ENABLED
      # - ZOO_ADMINSERVER_ENABLED
      # - ZOO_4LW_COMMANDS_WHITELIST
      # - ZOO_ADMINSERVER_ENABLED

  ch-server-1:
      image: yandex/clickhouse-server
      container_name: ch-server-1
      volumes:
        - ./clickhouse/config.xml:/etc/clickhouse-server/config.d/local.xml
        - ./clickhouse/macros1.xml:/etc/clickhouse-server/config.d/macros.xml
      ports:
        - 8123:8123
        - 9000:9000
      environment:
        TZ: "Asia/Shanghai"
      depends_on:
        mysql:
          condition: service_healthy
        redis:
          condition: service_healthy
        etcd:
          condition: service_healthy
        zookeeper:
          condition: service_healthy
  ch-server-2:
    image: yandex/clickhouse-server
    container_name: ch-server-2
    volumes:
      - ./clickhouse/config.xml:/etc/clickhouse-server/config.d/local.xml
      - ./clickhouse/macros2.xml:/etc/clickhouse-server/config.d/macros.xml
    ports:
      - 8124:8123
      - 9001:9000
    environment:
      TZ: "Asia/Shanghai"
    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
      etcd:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
  ch-server-3:
    image: yandex/clickhouse-server
    container_name: ch-server-3
    volumes:
      - ./clickhouse/config.xml:/etc/clickhouse-server/config.d/local.xml
      - ./clickhouse/macros3.xml:/etc/clickhouse-server/config.d/macros.xml
    ports:
      - 8125:8123
      - 9002:9000
    environment:
      TZ: "Asia/Shanghai"
    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
      etcd:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
  ch-server-4:
    image: yandex/clickhouse-server
    container_name: ch-server-4
    volumes:
      - ./clickhouse/config.xml:/etc/clickhouse-server/config.d/local.xml
      - ./clickhouse/macros4.xml:/etc/clickhouse-server/config.d/macros.xml
    ports:
      - 8126:8123
      - 9003:9000
    environment:
      TZ: "Asia/Shanghai"
    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
      etcd:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
  mongo1:
    image: mongo:4.4
    container_name: mongo1
    command: ["--replSet", "my-replica-set", "--bind_ip_all", "--port", "30001"]
    # volumes:
      # - ./data/mongo-1:/data/db
    ports:
      - 27017:30001
    healthcheck:
      test: test $$(echo "rs.initiate({_id:'my-replica-set',members:[{_id:0,host:\"mongo1:30001\"},{_id:1,host:\"mongo2:30002\"},{_id:2,host:\"mongo3:30003\"}]}).ok || rs.status().ok" | mongo --port 30001 --quiet) -eq 1
      interval: 10s
      start_period: 30s

  mongo2:
    image: mongo:4.4
    container_name: mongo2
    command: ["--replSet", "my-replica-set", "--bind_ip_all", "--port", "30002"]
    # volumes:
      # - ./data/mongo-2:/data/db
    ports:
      - 30002:30002

  mongo3:
    image: mongo:4.4
    container_name: mongo3
    command: ["--replSet", "my-replica-set", "--bind_ip_all", "--port", "30003"]
    # volumes:
      # - ./data/mongo-3:/data/db
    ports:
      - 30003:30003
  kafka:
    image: wurstmeister/kafka
    restart: always
    ports:
      - 9092:9092
    environment:
      - ZOOKEEPER_IP=zookeeper
      - KAFKA_BROKER_ID=1
      - KAFKA_LISTENERS=PLAINTEXT://:9092
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
    healthcheck:
      test: [ "CMD", "nc", "-z", "localhost", "9092" ]
      interval: 2s
      timeout: 5s
      retries: 30
  kafka-connector:
    image: liliankasem/kafka-connect:1.1.0
    container_name: kafka-connector
    hostname: connect
    container_name: connect
    restart: always
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_PLUGIN_PATH: /usr/share/java
    healthcheck:
      test: [ "CMD", "nc", "-z", "kafka-connector", "8083" ]
      interval: 2s
      timeout: 5s
      retries: 30
    volumes:
      - ../../third_party/mongo-kafka-1.2.0-all.jar:/usr/share/java/mongo-kafka-1.2.0-all.jar
    depends_on:
      kafka:
        condition: service_healthy
  canal-server:
    image: canal/canal-server
    container_name: canal-server
    restart: always
    environment:
      canal.admin.user: admin
      canal.admin.port: 11110
      canal.admin.passwd: 4ACFE3202A5FF5CF467898FC58AAB1D615029441
      canal.admin.manager: canal-admin:8089
  canal-admin:
    image: canal/canal-admin
    container_name: canal-admin
    ports:
      - 8089:8089
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://canal-admin:8089" ]
      interval: 30s
      timeout: 15s
      retries: 30
    restart: always    
    depends_on: 
      - canal-server
    volumes:
      - ../../third_party/canal_admin_conf/canal-template.properties:/home/admin/canal-admin/conf/canal-template.properties
